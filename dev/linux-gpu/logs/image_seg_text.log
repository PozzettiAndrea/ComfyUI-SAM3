Capturing execution frames: image_seg_text.json
  [Console-error] Failed to load resource: the server responded with a status of 404 (Not Found)
  [Console-error] Failed to load resource: the server responded with a status of 404 (Not Found)
  [Console-error] Failed to load resource: the server responded with a status of 404 (Not Found)
  [Console-error] Failed to load resource: the server responded with a status of 404 (Not Found)
  [Console-error] Failed to load resource: the server responded with a status of 404 (Not Found)
  [Console-error] Failed to load resource: the server responded with a status of 404 (Not Found)
  [Console-warning] [ComfyUI Deprecated] Importing from "scripts/ui.js" is deprecated and will be removed in v1.34.
  [Console-warning] [ComfyUI Notice] "scripts/widgets.js" is an internal module, not part of the public API. Future updates may break this import.
  [Console-warning] [ComfyUI Notice] "extensions/core/widgetInputs.js" is an internal module, not part of the public API. Future updates may break this import.
  [Console-warning] [ComfyUI Notice] "scripts/utils.js" is an internal module, not part of the public API. Future updates may break this import.
  [Console-warning] [MaskEditor] ComfyApp.open_maskeditor is deprecated. Plugins should migrate to using the command system or direct node context menu integration.
  [Console-warning] [ComfyUI Deprecated] Importing from "scripts/ui/components/buttonGroup.js" is deprecated and will be removed in v1.34.
  [Console-warning] [ComfyUI Deprecated] Importing from "scripts/ui/components/button.js" is deprecated and will be removed in v1.34.
  Frame 1 saved (frame_000.jpg, t=0.0s)
  Validating workflow...
  [ComfyUI] === /validate endpoint called ===
  [ComfyUI] Received JSON with keys: ['prompt']
  Queuing workflow for execution...
  [ComfyUI] Prompt has 5 nodes
  [ComfyUI]   Node 1: LoadImage
  [ComfyUI]     Inputs: {"image": "example_image.jpg"}
  [ComfyUI]   Node 3: PreviewImage
  [ComfyUI]     Inputs: {"images": ["13", 1]}
  [ComfyUI]   Node 4: MaskPreview
  [ComfyUI]     Inputs: {"mask": ["13", 0]}
  [ComfyUI]   Node 12: LoadSAM3Model
  [ComfyUI]     Inputs: {"precision": "auto", "attention": "auto", "compile": false}
  [ComfyUI]   Node 13: SAM3Grounding
  [ComfyUI]     Inputs: {"confidence_threshold": 0.2, "text_prompt": "person", "max_detections": -1, "sam3_model": ["12", 0], "image": ["1", 0]}
  [ComfyUI] Generated prompt_id: d5d4d34f-2bf2-460e-b37d-9d432bc35dd4
  [ComfyUI] Calling execution.validate_prompt()...
  [ComfyUI] Validation result: valid=True
  [ComfyUI] === Validation PASSED ===
  [ComfyUI] got prompt
  [ComfyUI] Loading model from: /home/shadeform/workspaces/SAM3-0057/ComfyUI/models/sam3/sam3.safetensors
  [ComfyUI] Sam3VideoPredictor using device: cuda:0
  [ComfyUI] SageAttention is not compatible with SAM3 (relative position bias modifies Q/K dimensions). Using attention_flash instead.
  [ComfyUI] setting max_num_objects=10000 and num_obj_for_compile=16
  [ComfyUI] Loading checkpoint: /home/shadeform/workspaces/SAM3-0057/ComfyUI/models/sam3/sam3.safetensors
  [ComfyUI] Added 309 keys for detector.inst_interactive_predictor
  Frame 2 saved (frame_001.jpg, t=2.4s)
  [capture-periodic] freeze=3ms shot=165ms unfreeze=15ms saved=True
  [ComfyUI] Model ready (1756.7 MB)
  [ComfyUI] Requested to load Sam3VideoInferenceWithInstanceInteractivity
  [ComfyUI] Running text-based detection
  [ComfyUI]   Text prompt: 'person'
  [ComfyUI] Confidence threshold: 0.2
  [ComfyUI] Image size: (1280, 720)
  [ComfyUI] [DEBUG] set_image: input shape=torch.Size([1, 3, 1008, 1008]), dtype=torch.bfloat16, min=-1.0000, max=1.0000, device=cuda:0
  [ComfyUI] Backbone.forward_image IN:   samples=torch.bfloat16 [1, 3, 1008, 1008] min=-1.0000 max=1.0000
  [ComfyUI] FPN_Neck.forward IN:   input=torch.bfloat16 [1, 3, 1008, 1008] min=-1.0000 max=1.0000
  [ComfyUI] ViT.forward IN:   x=torch.bfloat16 [1, 3, 1008, 1008] min=-1.0000 max=1.0000
  [ComfyUI] attention backend: attention_flash | dtype: torch.bfloat16
  [ComfyUI] ViT.forward OUT:   features=[torch.bfloat16 [1, 1024, 72, 72]]
  [ComfyUI] FPN_Neck.forward after trunk:   trunk_out=[torch.bfloat16 [1, 1024, 72, 72]]
  [ComfyUI] FPN_Neck.forward OUT:   sam3_out=[torch.bfloat16 [1, 256, 288, 288], torch.bfloat16 [1, 256, 144, 144], torch.bfloat16 [1, 256, 72, 72], torch.bfloat16 [1, 256, 36, 36]]   sam3_pos=[torch.bfloat16 [1, 256, 288, 288], torch.bfloat16 [1, 256, 144, 144], torch.bfloat16 [1, 256, 72, 72], torch.bfloat16 [1, 256, 36, 36]]
  [ComfyUI] Backbone.forward_image OUT:   vision_features=torch.bfloat16 [1, 256, 72, 72] min=-5.7188 max=5.5000   backbone_fpn=[torch.bfloat16 [1, 256, 288, 288], torch.bfloat16 [1, 256, 144, 144], torch.bfloat16 [1, 256, 72, 72]]
  [ComfyUI] [DEBUG] set_image: backbone_out keys: ['vision_features', 'vision_pos_enc', 'backbone_fpn', 'sam2_backbone_out']
  [ComfyUI] [DEBUG]   backbone_fpn[0]: shape=torch.Size([1, 256, 288, 288]), dtype=torch.bfloat16, min=-0.2412, max=0.2031
  [ComfyUI] [DEBUG]   backbone_fpn[1]: shape=torch.Size([1, 256, 144, 144]), dtype=torch.bfloat16, min=-10.8750, max=11.7500
  [ComfyUI] [DEBUG]   backbone_fpn[2]: shape=torch.Size([1, 256, 72, 72]), dtype=torch.bfloat16, min=-5.7188, max=5.5000
  [ComfyUI] Adding text prompt...
  [ComfyUI] [DEBUG] set_text_prompt: prompt='person', device=cuda:0
  [ComfyUI] [DEBUG] language_features: shape=torch.Size([32, 1, 256]), dtype=torch.bfloat16, min=-3.5469, max=4.5000, mean=0.0557
  [ComfyUI] [DEBUG] language_mask: shape=torch.Size([1, 32]), dtype=torch.bool, num_valid=3, num_padding=29
  [ComfyUI] forward_grounding: after _encode_prompt:   prompt=torch.bfloat16 [33, 1, 256] min=-6.4062 max=10.1250   prompt_mask=torch.bool [1, 33] min=0.0000 max=1.0000
  [ComfyUI] _run_encoder inputs:   img_feats=[torch.bfloat16 [5184, 1, 256]]   prompt=torch.bfloat16 [33, 1, 256] min=-6.4062 max=10.1250   prompt_mask=torch.bool [1, 33] min=0.0000 max=1.0000
  [ComfyUI] Encoder.forward IN:   src=[torch.bfloat16 [5184, 1, 256]]   prompt=torch.bfloat16 [33, 1, 256] min=-6.4062 max=10.1250
  Frame 3 saved (frame_002.jpg, t=4.7s)
  [capture-periodic] freeze=5ms shot=224ms unfreeze=7ms saved=True
  [ComfyUI] Encoder.forward OUT:   memory=torch.bfloat16 [5184, 1, 256] min=-20.0000 max=11.4375   pos_embed=torch.bfloat16 [5184, 1, 256] min=-1.0000 max=1.0000   memory_text=torch.bfloat16 [33, 1, 256] min=-6.4062 max=10.1250
  [ComfyUI] _run_encoder output:   memory=torch.bfloat16 [5184, 1, 256] min=-20.0000 max=11.4375   pos_embed=torch.bfloat16 [5184, 1, 256] min=-1.0000 max=1.0000
  [ComfyUI] forward_grounding: after _run_encoder:   encoder_hidden_states=torch.bfloat16 [5184, 1, 256] min=-20.0000 max=11.4375   pos_embed=torch.bfloat16 [5184, 1, 256] min=-1.0000 max=1.0000
  [ComfyUI] _run_decoder inputs:   tgt=torch.bfloat16 [200, 1, 256] min=-0.6758 max=0.5938   memory=torch.bfloat16 [5184, 1, 256] min=-20.0000 max=11.4375   pos_embed=torch.bfloat16 [5184, 1, 256] min=-1.0000 max=1.0000   prompt=torch.bfloat16 [33, 1, 256] min=-6.4062 max=10.1250   prompt_mask=torch.bool [1, 33] min=0.0000 max=1.0000
  [ComfyUI] Decoder.forward IN:   tgt=torch.bfloat16 [200, 1, 256] min=-0.6758 max=0.5938   memory=torch.bfloat16 [5184, 1, 256] min=-20.0000 max=11.4375   memory_text=torch.bfloat16 [33, 1, 256] min=-6.4062 max=10.1250
  [ComfyUI] Decoder.forward OUT:   output=torch.bfloat16 [6, 200, 1, 256] min=-6.7500 max=6.8125   ref_boxes=torch.bfloat16 [6, 200, 1, 4] min=0.0089 max=0.9805   presence=torch.bfloat16 [6, 1, 1] min=-1.0859 max=-0.9336
  [ComfyUI] _run_decoder output:   hs=torch.bfloat16 [6, 200, 1, 256] min=-6.7500 max=6.8125   reference_boxes=torch.bfloat16 [6, 200, 1, 4] min=0.0089 max=0.9805   dec_presence_out=torch.bfloat16 [6, 1, 1] min=-1.0859 max=-0.9336
  [ComfyUI] _update_scores_and_boxes:   hs=torch.bfloat16 [6, 1, 200, 256] min=-6.7500 max=6.8125   prompt=torch.bfloat16 [33, 1, 256] min=-6.4062 max=10.1250   prompt_mask=torch.bool [1, 33] min=0.0000 max=1.0000
  [ComfyUI] _update_scores: outputs_class:   outputs_class=torch.bfloat16 [6, 1, 200, 1] min=-9.6250 max=2.7188
  [ComfyUI] _update_scores: before joint_box_scores:   outputs_class_pre=torch.bfloat16 [6, 1, 200, 1] min=-9.6250 max=2.7188   dec_presence_out=torch.bfloat16 [6, 1, 1] min=-1.0859 max=-0.9336
  [ComfyUI] _update_scores: after joint_box_scores:   outputs_class_post=torch.bfloat16 [6, 1, 200, 1] min=-6.9062 max=-1.0547
  [ComfyUI] forward_grounding: after _run_decoder:   encoder_hidden_states=torch.bfloat16 [5184, 1, 256] min=-20.0000 max=11.4375   pred_boxes=torch.bfloat16 [1, 200, 4] min=0.0094 max=0.9844
  [ComfyUI] SegHead.forward IN:   backbone_feats=[torch.bfloat16 [1, 256, 288, 288], torch.bfloat16 [1, 256, 144, 144], torch.bfloat16 [1, 256, 72, 72]]   obj_queries=torch.bfloat16 [6, 1, 200, 256] min=-6.7500 max=6.8125   encoder_hidden_states=torch.bfloat16 [5184, 1, 256] min=-20.0000 max=11.4375
  [ComfyUI] SegHead.forward OUT:   pred_masks=torch.bfloat16 [1, 200, 288, 288] min=-123.5000 max=13.1875   semantic_seg=torch.bfloat16 [1, 1, 288, 288] min=-35.5000 max=13.5000   presence_logit=None
  [ComfyUI] forward_grounding: after _run_segmentation_heads:   pred_logits=torch.bfloat16 [1, 200, 1] min=-6.9062 max=-1.1406   pred_masks=torch.bfloat16 [1, 200, 288, 288] min=-123.5000 max=13.1875   pred_boxes=torch.bfloat16 [1, 200, 4] min=0.0094 max=0.9844
  [ComfyUI] [DEBUG] forward_grounding output keys: ['encoder_hidden_states', 'prev_encoder_out', 'presence_feats', 'queries', 'presence_logit_dec', 'pred_logits', 'pred_boxes', 'pred_boxes_xyxy', 'pred_masks', 'semantic_seg', 'presence_logit']
  [ComfyUI] [DEBUG] pred_logits shape: torch.Size([1, 200, 1]), min: -6.9062, max: -1.1406
  [ComfyUI] [DEBUG] pred_boxes shape: torch.Size([1, 200, 4])
  [ComfyUI] [DEBUG] pred_masks shape: torch.Size([1, 200, 288, 288])
  [ComfyUI] [DEBUG] presence_logit_dec: torch.Size([1, 1]), val=[-1.0546875], sigmoid=[0.2578125]
  [ComfyUI] [DEBUG] out_probs (joint_box_scores, no double presence): min=0.0010, max=0.2422
  [ComfyUI] [DEBUG] confidence_threshold: 0.2
  [ComfyUI] [DEBUG] detections above threshold: 6 / 200
  [ComfyUI] [DEBUG] top-10 probs: ['0.2422', '0.2383', '0.2363', '0.2354', '0.2354', '0.2295', '0.0233', '0.0192', '0.0188', '0.0186']
  [ComfyUI] [DEBUG] after threshold: 6 detections
  [ComfyUI] [DEBUG] after NMS (iou_thresh=0.5): 6 detections (suppressed 0)
  [ComfyUI] Found 6 detections above threshold 0.2
  [ComfyUI] Sorting 6 detections by score...
  [ComfyUI] Creating visualization...
  [ComfyUI] Detection complete: 6 masks
  [capture-loop] iter=50 t=7.0s state={'complete': False, 'error': None, 'executedCount': 0, 'wsState': 1} eval_ms=7
  [ComfyUI] Prompt executed in 6.90 seconds
  Frame 4 saved (frame_003.jpg, t=7.0s)
  [capture-periodic] freeze=23ms shot=191ms unfreeze=5ms saved=True
  Node executed (2 total), capturing...
  Frame 5 saved (frame_004.jpg, t=7.8s)
  [capture-node] freeze=33ms shot=247ms unfreeze=11ms saved=True
  Execution complete (t=8.2s)
  Captured 5 unique frames over 8.26s
  [debug] iframes: 0
  [timing] trigger_3d_previews: 0.0s
  Saved high-quality screenshot: image_seg_text_executed.png
  [timing] final_screenshot: 1.5s
    Captured 6 video frames
    VRAM log: /home/shadeform/vramlogs/SAM3_image_seg_text_20260226_005943.csv