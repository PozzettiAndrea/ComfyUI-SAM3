Capturing execution frames: scene_seg.json
  [Console-error] Failed to load resource: the server responded with a status of 404 (Not Found)
  [Console-error] Failed to load resource: the server responded with a status of 404 (Not Found)
  [Console-error] Failed to load resource: the server responded with a status of 404 (Not Found)
  [Console-error] Failed to load resource: the server responded with a status of 404 (Not Found)
  [Console-error] Failed to load resource: the server responded with a status of 404 (Not Found)
  [Console-error] Failed to load resource: the server responded with a status of 404 (Not Found)
  [Console-warning] [ComfyUI Notice] "extensions/core/widgetInputs.js" is an internal module, not part of the public API. Future updates may break this import.
  [Console-warning] [ComfyUI Notice] "scripts/utils.js" is an internal module, not part of the public API. Future updates may break this import.
  [Console-warning] [ComfyUI Deprecated] Importing from "scripts/ui.js" is deprecated and will be removed in v1.34.
  [Console-warning] [ComfyUI Notice] "scripts/widgets.js" is an internal module, not part of the public API. Future updates may break this import.
  [Console-warning] [MaskEditor] ComfyApp.open_maskeditor is deprecated. Plugins should migrate to using the command system or direct node context menu integration.
  [Console-warning] [ComfyUI Deprecated] Importing from "scripts/ui/components/buttonGroup.js" is deprecated and will be removed in v1.34.
  [Console-warning] [ComfyUI Deprecated] Importing from "scripts/ui/components/button.js" is deprecated and will be removed in v1.34.
  Frame 1 saved (frame_000.jpg, t=0.0s)
  Validating workflow...
  [ComfyUI] === /validate endpoint called ===
  Queuing workflow for execution...
  [ComfyUI] Received JSON with keys: ['prompt']
  [ComfyUI] Prompt has 10 nodes
  [ComfyUI]   Node 1: LoadImage
  [ComfyUI]     Inputs: {"image": "image.png"}
  [ComfyUI]   Node 3: PreviewImage
  [ComfyUI]     Inputs: {"images": ["21", 1]}
  [ComfyUI]   Node 4: MaskPreview
  [ComfyUI]     Inputs: {"mask": ["21", 0]}
  [ComfyUI]   Node 12: LoadSAM3Model
  [ComfyUI]     Inputs: {"precision": "auto", "attention": "auto", "compile": false}
  [ComfyUI]   Node 20: SAM3MultiRegionCollector
  [ComfyUI]     Inputs: {"multi_prompts_store": "[{\"positive_points\":[{\"x\":5052.74953932266,\"y\":2847.0600693869637},{\"x\":4932.510591392486,\"y\":2953.9413227753366},{\"x\":4718.752461738845,\"y\":2967.301479448883},{
  [ComfyUI]   Node 21: SAM3MultipromptSegmentation
  [ComfyUI]     Inputs: {"refinement_iterations": 2, "use_multimask": false, "sam3_model": ["12", 0], "image": ["1", 0], "multi_prompts": ["20", 0]}
  [ComfyUI]   Node 22: MultibandFromBatch
  [ComfyUI]     Inputs: {"images": ["1", 0], "masks": ["21", 0]}
  [ComfyUI]   Node 23: MultibandSave
  [ComfyUI]     Inputs: {"file_path": "output/multiband", "format": "npz", "multiband": ["26", 0]}
  [ComfyUI]   Node 24: MultibandPreview
  [ComfyUI]     Inputs: {"channel_index": 0, "multiband": ["26", 0]}
  [ComfyUI]   Node 26: MultibandResize
  [ComfyUI]     Inputs: {"upscale_method": "nearest-exact", "width": 900, "height": 600, "crop": "center", "multiband": ["22", 0]}
  [ComfyUI] Generated prompt_id: 76fb6555-c11a-48fc-8fb7-e6866b54dae1
  [ComfyUI] Calling execution.validate_prompt()...
  [ComfyUI] Validation result: valid=True
  [ComfyUI] === Validation PASSED ===
  [ComfyUI] got prompt
  [ComfyUI] WARNING: 'NoneType' object has no attribute 'shape'
  Frame 2 saved (frame_001.jpg, t=3.0s)
  [capture-periodic] freeze=4ms shot=179ms unfreeze=6ms saved=True
  [ComfyUI] CACHE MISS - computing new result for key=35316a77
  [ComfyUI] Image dimensions: 6720x4480
  [ComfyUI] Processing 7 prompt regions
  [ComfyUI]   Prompt 0: 10 pos pts, 0 neg pts, 1 pos boxes, 0 neg boxes
  [ComfyUI]   Prompt 1: 7 pos pts, 16 neg pts, 1 pos boxes, 0 neg boxes
  [ComfyUI]   Prompt 2: 7 pos pts, 14 neg pts, 1 pos boxes, 0 neg boxes
  [ComfyUI]   Prompt 3: 1 pos pts, 0 neg pts, 1 pos boxes, 0 neg boxes
  [ComfyUI]   Prompt 4: 1 pos pts, 0 neg pts, 1 pos boxes, 0 neg boxes
  [ComfyUI]   Prompt 5: 13 pos pts, 0 neg pts, 1 pos boxes, 0 neg boxes
  [ComfyUI]   Prompt 6: 6 pos pts, 0 neg pts, 1 pos boxes, 0 neg boxes
  [ComfyUI] Output: 7 non-empty prompts
  [ComfyUI] Loading model from: /home/shadeform/workspaces/SAM3-0057/ComfyUI/models/sam3/sam3.safetensors
  [ComfyUI] Sam3VideoPredictor using device: cuda:0
  [ComfyUI] SageAttention is not compatible with SAM3 (relative position bias modifies Q/K dimensions). Using attention_flash instead.
  Node executed (1 total), capturing...
  [ComfyUI] setting max_num_objects=10000 and num_obj_for_compile=16
  [ComfyUI] Loading checkpoint: /home/shadeform/workspaces/SAM3-0057/ComfyUI/models/sam3/sam3.safetensors
  [ComfyUI] Added 309 keys for detector.inst_interactive_predictor
  Frame 3 saved (frame_002.jpg, t=7.8s)
  [capture-node] freeze=110ms shot=406ms unfreeze=8ms saved=True
  [ComfyUI] Model ready (1756.7 MB)
  [ComfyUI] Requested to load Sam3VideoInferenceWithInstanceInteractivity
  Frame 4 saved (frame_003.jpg, t=10.5s)
  [ComfyUI] Image size: (6720, 4480)
  [capture-periodic] freeze=4ms shot=204ms unfreeze=5ms saved=True
  [ComfyUI] Processing 7 prompt regions
  [ComfyUI] [DEBUG] set_image: input shape=torch.Size([1, 3, 1008, 1008]), dtype=torch.bfloat16, min=-1.0000, max=0.9531, device=cuda:0
  [ComfyUI] Backbone.forward_image IN:   samples=torch.bfloat16 [1, 3, 1008, 1008] min=-1.0000 max=0.9531
  [ComfyUI] FPN_Neck.forward IN:   input=torch.bfloat16 [1, 3, 1008, 1008] min=-1.0000 max=0.9531
  [ComfyUI] ViT.forward IN:   x=torch.bfloat16 [1, 3, 1008, 1008] min=-1.0000 max=0.9531
  [ComfyUI] attention backend: attention_flash | dtype: torch.bfloat16
  [ComfyUI] ViT.forward OUT:   features=[torch.bfloat16 [1, 1024, 72, 72]]
  [ComfyUI] FPN_Neck.forward after trunk:   trunk_out=[torch.bfloat16 [1, 1024, 72, 72]]
  [ComfyUI] FPN_Neck.forward OUT:   sam3_out=[torch.bfloat16 [1, 256, 288, 288], torch.bfloat16 [1, 256, 144, 144], torch.bfloat16 [1, 256, 72, 72], torch.bfloat16 [1, 256, 36, 36]]   sam3_pos=[torch.bfloat16 [1, 256, 288, 288], torch.bfloat16 [1, 256, 144, 144], torch.bfloat16 [1, 256, 72, 72], torch.bfloat16 [1, 256, 36, 36]]
  [ComfyUI] Backbone.forward_image OUT:   vision_features=torch.bfloat16 [1, 256, 72, 72] min=-6.0000 max=6.0625   backbone_fpn=[torch.bfloat16 [1, 256, 288, 288], torch.bfloat16 [1, 256, 144, 144], torch.bfloat16 [1, 256, 72, 72]]
  [ComfyUI] [DEBUG] set_image: backbone_out keys: ['vision_features', 'vision_pos_enc', 'backbone_fpn', 'sam2_backbone_out']
  [ComfyUI] [DEBUG]   backbone_fpn[0]: shape=torch.Size([1, 256, 288, 288]), dtype=torch.bfloat16, min=-0.2930, max=0.2539
  [ComfyUI] [DEBUG]   backbone_fpn[1]: shape=torch.Size([1, 256, 144, 144]), dtype=torch.bfloat16, min=-11.8750, max=12.4375
  [ComfyUI] [DEBUG]   backbone_fpn[2]: shape=torch.Size([1, 256, 72, 72]), dtype=torch.bfloat16, min=-6.0000, max=6.0625
  [ComfyUI] Processing prompt region 1/7
  [ComfyUI]   Points: 10, Box: Yes
  [ComfyUI]   Mask score: 0.9805
  [ComfyUI] Processing prompt region 2/7
  [ComfyUI]   Points: 23, Box: Yes
  [ComfyUI]   Mask score: 0.9414
  [ComfyUI] Processing prompt region 3/7
  [ComfyUI]   Points: 21, Box: Yes
  [capture-loop] iter=50 t=12.1s state={'complete': False, 'error': None, 'executedCount': 1, 'wsState': 1} eval_ms=5
  [ComfyUI]   Mask score: 0.9023
  [ComfyUI] Processing prompt region 4/7
  [ComfyUI]   Points: 1, Box: Yes
  [ComfyUI]   Mask score: 0.9766
  [ComfyUI] Processing prompt region 5/7
  [ComfyUI]   Points: 1, Box: Yes
  [ComfyUI]   Mask score: 0.9844
  [ComfyUI] Processing prompt region 6/7
  [ComfyUI]   Points: 13, Box: Yes
  [ComfyUI]   Mask score: 0.9883
  [ComfyUI] Processing prompt region 7/7
  [ComfyUI]   Points: 6, Box: Yes
  [ComfyUI]   Mask score: 0.9688
  [ComfyUI] Generated 7 masks
  [capture-loop] iter=100 t=18.8s state={'complete': False, 'error': None, 'executedCount': 1, 'wsState': 1} eval_ms=4
  [capture-loop] iter=150 t=24.8s state={'complete': False, 'error': None, 'executedCount': 1, 'wsState': 1} eval_ms=5
  [capture-loop] iter=200 t=31.0s state={'complete': False, 'error': None, 'executedCount': 1, 'wsState': 1} eval_ms=8
  Frame 5 saved (frame_004.jpg, t=33.3s)
  [capture-periodic] freeze=4ms shot=212ms unfreeze=10ms saved=True
  [capture-loop] iter=250 t=37.2s state={'complete': False, 'error': None, 'executedCount': 1, 'wsState': 1} eval_ms=4
  Node executed (2 total), capturing...
  Frame 6 saved (frame_005.jpg, t=38.2s)
  [capture-node] freeze=247ms shot=1271ms unfreeze=9ms saved=True
  [capture-loop] iter=300 t=45.6s state={'complete': False, 'error': None, 'executedCount': 2, 'wsState': 1} eval_ms=5
  Node executed (3 total), capturing...
  Frame 7 saved (frame_006.jpg, t=48.9s)
  [capture-node] freeze=4ms shot=204ms unfreeze=7ms saved=True
  Node executed (4 total), capturing...
  Frame 8 saved (frame_007.jpg, t=50.6s)
  [capture-node] freeze=4ms shot=243ms unfreeze=11ms saved=True
  [ComfyUI] Prompt executed in 50.20 seconds
  Execution complete (t=51.1s)
  Captured 8 unique frames over 51.13s
  [debug] iframes: 0
  [timing] trigger_3d_previews: 0.0s
  Saved high-quality screenshot: scene_seg_executed.png
  [timing] final_screenshot: 2.4s
    Captured 9 video frames
    VRAM log: /home/shadeform/vramlogs/SAM3_scene_seg_20260226_010003.csv